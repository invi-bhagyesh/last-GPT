{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GPT model","metadata":{}},{"cell_type":"code","source":"import torch\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nd_model = 512\nn_heads = 4  \nn_layers = 2  \ncontext_length = 256\ndropout = 0.1\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, n_heads: int):\n        super().__init__()\n        self.n_heads = n_heads\n        self.head_dim = d_model // n_heads\n        assert d_model % n_heads == 0\n\n        # Combined QKV projection (more efficient)\n        self.qkv = nn.Linear(d_model, 3 * d_model)\n        self.proj = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('mask', torch.tril(torch.ones(context_length, context_length)))\n\n    def forward(self, x):\n        B, T, C = x.shape\n        qkv = self.qkv(x).split(d_model, dim=2)\n        # Process Q, K, V\n        q, k, v = [y.view(B, T, self.n_heads, self.head_dim).transpose(1, 2) for y in qkv]\n        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n        \n        # Apply causal mask\n        att = att.masked_fill(self.mask[:T,:T] == 0, float('-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.dropout(att)\n        \n        y = (att @ v).transpose(1, 2).contiguous().view(B, T, C)\n        return self.dropout(self.proj(y))\n\nclass GPTBlock(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(d_model)\n        self.attn = MultiHeadAttention(d_model, n_heads)\n        self.ln2 = nn.LayerNorm(d_model)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.GELU(),\n            nn.Linear(4 * d_model, d_model),\n            nn.Dropout(dropout)\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Pre-LN architecture (original GPT-2 style)\n        x = x + self.attn(self.ln1(x))\n        x = x + self.ffn(self.ln2(x))\n        return x\n\nclass GPT(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.context_length = context_length\n        self.wte = nn.Embedding(vocab_size, d_model)\n        self.wpe = nn.Embedding(context_length, d_model)  # Learned positional embeddings\n        self.blocks = nn.Sequential(*[GPTBlock(d_model, n_heads) for _ in range(n_layers)])\n        self.ln_f = nn.LayerNorm(d_model)\n        self.head = nn.Linear(d_model, vocab_size)\n        \n        # GPT-2 style initialization\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.size()\n        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n        tok_emb = self.wte(idx)\n        pos_emb = self.wpe(pos)\n        x = tok_emb + pos_emb\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.head(x)\n        \n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -self.context_length:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:03:20.779099Z","iopub.execute_input":"2025-04-01T08:03:20.779386Z","iopub.status.idle":"2025-04-01T08:03:23.988743Z","shell.execute_reply.started":"2025-04-01T08:03:20.779355Z","shell.execute_reply":"2025-04-01T08:03:23.987845Z"}},"outputs":[{"name":"stdout","text":"before the loop!!!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"checkpoint_path = \"/kaggle/input/epoch-2-gpt/pytorch/default/1/gpt_model_epoch2.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:03:24.119619Z","iopub.execute_input":"2025-04-01T08:03:24.119861Z","iopub.status.idle":"2025-04-01T08:03:24.123858Z","shell.execute_reply.started":"2025-04-01T08:03:24.119841Z","shell.execute_reply":"2025-04-01T08:03:24.123089Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"vocab_size = 50257  \nmodel = GPT(vocab_size)\nmodel.load_state_dict(torch.load(checkpoint_path, map_location=torch.device('cpu')))\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nprompt = \"Maths is hard\"\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Change this if you used a different tokenizer\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\nprint(\"Tokenisation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:03:27.538419Z","iopub.execute_input":"2025-04-01T08:03:27.538595Z","iopub.status.idle":"2025-04-01T08:03:34.192714Z","shell.execute_reply.started":"2025-04-01T08:03:27.538579Z","shell.execute_reply":"2025-04-01T08:03:34.191822Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"198b8b3259d64402a1a97ab5b2b381c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6d35ee869ff43b697fb91edfe19e7c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5476088bd4c84bc882d764e2aa7e796c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33afb6bd6fdf4997a0bf6396b31d88b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37733505a3254191a2eda3c18eba7ad2"}},"metadata":{}},{"name":"stdout","text":"Tokenisation\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"output_ids = model.generate(input_ids, max_new_tokens=100)  # Use max_new_tokens\ngenerated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:03:34.193728Z","iopub.execute_input":"2025-04-01T08:03:34.194197Z","iopub.status.idle":"2025-04-01T08:03:38.268714Z","shell.execute_reply.started":"2025-04-01T08:03:34.194172Z","shell.execute_reply":"2025-04-01T08:03:38.267911Z"}},"outputs":[{"name":"stdout","text":"Maths is hard ; have a minor role . Typically , sperm compartment consists of several , centering , and adult movements . About 90 % of olfactoryups have problems ( transjm ) making more intruded polemic when multiples of Iowa reserves , some have fewer or larger cells than it is . \n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:03:38.269436Z","iopub.execute_input":"2025-04-01T08:03:38.269671Z","iopub.status.idle":"2025-04-01T08:03:42.439969Z","shell.execute_reply.started":"2025-04-01T08:03:38.269644Z","shell.execute_reply":"2025-04-01T08:03:42.439080Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Squad Dataset","metadata":{}},{"cell_type":"code","source":"print(\"Preprocessing dataset for QA\")\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# Load a QA dataset \ndataset = load_dataset(\"squad\")  # You can replace this with a WikiText-derived QA dataset\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token  # Set EOS token as PAD token\n\n# Define preprocessing function for QA\ndef preprocess_function(examples):\n    inputs = [\"Q: \" + q + \" A:\" for q in examples[\"question\"]]\n    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n\n    # Setup the targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer([answer[\"text\"][0] if len(answer[\"text\"]) > 0 else \"\" for answer in examples[\"answers\"]], \n                   max_length=256, truncation=True, padding=\"max_length\")\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply preprocessing\nprocessed_dataset = dataset.map(preprocess_function, batched=True)\n\nprint(\"Dataset preprocessing complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:03:42.442713Z","iopub.execute_input":"2025-04-01T08:03:42.442960Z","iopub.status.idle":"2025-04-01T08:04:14.999398Z","shell.execute_reply.started":"2025-04-01T08:03:42.442939Z","shell.execute_reply":"2025-04-01T08:04:14.998510Z"}},"outputs":[{"name":"stdout","text":"Preprocessing dataset for QA\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6477e6da21194acf9f5cc0e182bbdb83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dae035fa52ff4a1f85345b212cc5b5a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5566908b528442ebb0adfabb97d8203b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c825e98ff4f4f1493538fff8c8a173d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e9738620b14454c83a65f391eb9443e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5ae1d23f64e4e9881608aac6e10b2d1"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e652eefe62b8497f842ab726a58a410a"}},"metadata":{}},{"name":"stdout","text":"Dataset preprocessing complete!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:04:15.012254Z","iopub.execute_input":"2025-04-01T08:04:15.012461Z","iopub.status.idle":"2025-04-01T08:04:15.823979Z","shell.execute_reply.started":"2025-04-01T08:04:15.012432Z","shell.execute_reply":"2025-04-01T08:04:15.823105Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_dataset = TensorDataset(\n    torch.tensor(processed_dataset[\"train\"][\"input_ids\"]),\n    torch.tensor(processed_dataset[\"train\"][\"attention_mask\"]),\n    torch.tensor(processed_dataset[\"train\"][\"labels\"])\n)\nval_dataset = TensorDataset(\n    torch.tensor(processed_dataset[\"validation\"][\"input_ids\"]),\n    torch.tensor(processed_dataset[\"validation\"][\"attention_mask\"]),\n    torch.tensor(processed_dataset[\"validation\"][\"labels\"])\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:04:15.824795Z","iopub.execute_input":"2025-04-01T08:04:15.825119Z","iopub.status.idle":"2025-04-01T08:05:01.315099Z","shell.execute_reply.started":"2025-04-01T08:04:15.825088Z","shell.execute_reply":"2025-04-01T08:05:01.314146Z"}},"outputs":[{"name":"stdout","text":"Converting to TensorDataset- training\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(\"Dataloaders\")\nbatch_size = 8  # Smaller batch size due to longer sequences\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last= True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, drop_last= True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:05:01.316251Z","iopub.execute_input":"2025-04-01T08:05:01.316523Z","iopub.status.idle":"2025-04-01T08:05:01.321760Z","shell.execute_reply.started":"2025-04-01T08:05:01.316499Z","shell.execute_reply":"2025-04-01T08:05:01.320881Z"}},"outputs":[{"name":"stdout","text":"Dataloaders\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"pip install wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:05:01.322627Z","iopub.execute_input":"2025-04-01T08:05:01.322914Z","iopub.status.idle":"2025-04-01T08:05:04.571125Z","shell.execute_reply.started":"2025-04-01T08:05:01.322881Z","shell.execute_reply":"2025-04-01T08:05:04.570184Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.0a2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.29.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T08:05:04.572128Z","iopub.execute_input":"2025-04-01T08:05:04.572402Z","iopub.status.idle":"2025-04-01T08:05:08.198391Z","shell.execute_reply.started":"2025-04-01T08:05:04.572378Z","shell.execute_reply":"2025-04-01T08:05:08.197498Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print(\"wandb\")\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\n\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"add\")\n\nwandb.login(key=wandb_api_key)\nwandb.init(project=\"gpt-fine_10\", config={\n    \"learning_rate\": 2e-5,\n    \"num_epochs\": 3,\n    \"warmup_steps\": 500\n})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T09:13:49.372124Z","iopub.execute_input":"2025-04-01T09:13:49.372433Z","iopub.status.idle":"2025-04-01T09:13:56.278101Z","shell.execute_reply.started":"2025-04-01T09:13:49.372408Z","shell.execute_reply":"2025-04-01T09:13:56.277393Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"wandb\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250401_091349-n7z997qk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/invi-bhagyesh-manipal/gpt-fine_10/runs/n7z997qk' target=\"_blank\">sunny-fog-1</a></strong> to <a href='https://wandb.ai/invi-bhagyesh-manipal/gpt-fine_10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/invi-bhagyesh-manipal/gpt-fine_10' target=\"_blank\">https://wandb.ai/invi-bhagyesh-manipal/gpt-fine_10</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/invi-bhagyesh-manipal/gpt-fine_10/runs/n7z997qk' target=\"_blank\">https://wandb.ai/invi-bhagyesh-manipal/gpt-fine_10/runs/n7z997qk</a>"},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/invi-bhagyesh-manipal/gpt-fine_10/runs/n7z997qk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x795ddbb38910>"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"# Fine-tuning","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nfrom transformers import get_linear_schedule_with_warmup\nimport evaluate\nimport wandb\n\n# Initialize SQuAD metric\nsquad_metric = evaluate.load(\"squad\")\n\nprint(\"Before training\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nlearning_rate = 2e-5\nnum_epochs = 3\nwarmup_steps = 500\ntotal_steps = len(train_dataloader) * num_epochs\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n\nprint(\"Entering training loop\")\nfor epoch in range(num_epochs):\n    print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n    model.train()\n    total_loss = 0\n\n    # Training loop (unchanged)\n    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\")):\n        if len(batch) != 3:\n            raise ValueError(f\"Unexpected batch size at step {step}: expected 3 items, got {len(batch)}\")\n        input_ids, attention_mask, labels = batch\n        input_ids, labels = input_ids.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, targets=labels)\n        loss = outputs[1]\n        loss = loss.mean()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n        wandb.log({\"train_loss_batch\": loss.item(), \"learning_rate\": scheduler.get_last_lr()[0]})\n    \n    avg_train_loss = total_loss / len(train_dataloader)\n\n    # Validation loop with SQuAD metrics\n    model.eval()\n    total_val_loss = 0\n    predictions = []\n    references = []\n\n    with torch.no_grad():\n        for step, batch in enumerate(tqdm(val_dataloader, desc=f\"Validating Epoch {epoch+1}\")):\n            if len(batch) != 3:\n                raise ValueError(f\"Unexpected batch size at step {step}: expected 3 items, got {len(batch)}\")\n            input_ids, attention_mask, labels = batch\n            input_ids, labels = input_ids.to(device), labels.to(device)\n            \n            # Get model outputs\n            outputs = model(input_ids, targets=labels)\n            loss = outputs[1]\n            loss = loss.mean()\n            total_val_loss += loss.item()\n            \n            # Generate predictions (using argmax for simplicity)\n            preds = torch.argmax(outputs[0], dim=-1)\n            pred_answers = tokenizer.batch_decode(preds, skip_special_tokens=True)\n            true_answers = tokenizer.batch_decode(labels, skip_special_tokens=True)\n            \n            # Prepare for SQuAD metric\n            for i in range(len(input_ids)):\n                question_id = f\"epoch{epoch}_batch{step}_item{i}\"\n                predictions.append({\n                    \"id\": question_id,\n                    \"prediction_text\": pred_answers[i]\n                })\n                references.append({\n                    \"id\": question_id,\n                    \"answers\": {\n                        \"text\": [true_answers[i]],\n                        \"answer_start\": [0]  # Dummy position\n                    }\n                })\n\n    avg_val_loss = total_val_loss / len(val_dataloader)\n    squad_results = squad_metric.compute(predictions=predictions, references=references)\n    \n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n    print(f\"Train loss: {avg_train_loss:.4f} | Val loss: {avg_val_loss:.4f}\")\n    print(f\"SQuAD EM: {squad_results['exact_match']:.2f} | SQuAD F1: {squad_results['f1']:.2f}\")\n\n    wandb.log({\n        \"epoch\": epoch + 1,\n        \"train_loss\": avg_train_loss,\n        \"val_loss\": avg_val_loss,\n        \"squad_em\": squad_results['exact_match'],\n        \"squad_f1\": squad_results['f1'],\n        \"learning_rate\": scheduler.get_last_lr()[0]\n    })\n\n    torch.save(model.state_dict(), f\"gpt_model_epoch{epoch+1}.pth\")\n\nwandb.finish()\nprint(\"Training done\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T09:13:56.279026Z","iopub.execute_input":"2025-04-01T09:13:56.279294Z","iopub.status.idle":"2025-04-01T09:58:03.896839Z","shell.execute_reply.started":"2025-04-01T09:13:56.279273Z","shell.execute_reply":"2025-04-01T09:58:03.896188Z"}},"outputs":[{"name":"stdout","text":"Before training\nEntering training loop\nStarting epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1: 100%|██████████| 10949/10949 [14:04<00:00, 12.96it/s]\nValidating Epoch 1: 100%|██████████| 1321/1321 [00:35<00:00, 36.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/3\nTrain loss: 0.1608 | Val loss: 0.1669\nSQuAD EM: 0.00 | SQuAD F1: 0.32\nStarting epoch 2/3\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|██████████| 10949/10949 [14:04<00:00, 12.97it/s]\nValidating Epoch 2: 100%|██████████| 1321/1321 [00:35<00:00, 36.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/3\nTrain loss: 0.1546 | Val loss: 0.1691\nSQuAD EM: 0.00 | SQuAD F1: 0.22\nStarting epoch 3/3\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3: 100%|██████████| 10949/10949 [14:04<00:00, 12.96it/s]\nValidating Epoch 3: 100%|██████████| 1321/1321 [00:35<00:00, 36.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/3\nTrain loss: 0.1504 | Val loss: 0.1711\nSQuAD EM: 0.00 | SQuAD F1: 0.50\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>learning_rate</td><td>▃▃████▇▇▆▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>squad_em</td><td>▁▁▁</td></tr><tr><td>squad_f1</td><td>▄▁█</td></tr><tr><td>train_loss</td><td>█▄▁</td></tr><tr><td>train_loss_batch</td><td>▂▄▂▃▃▆▃▅█▁▄▄▂▁▄▃▄▃▂▁▂▁▁▁▂▆▂▄▃▁▂▅▂▂▅▄▂▂▅▅</td></tr><tr><td>val_loss</td><td>▁▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>learning_rate</td><td>0</td></tr><tr><td>squad_em</td><td>0</td></tr><tr><td>squad_f1</td><td>0.49947</td></tr><tr><td>train_loss</td><td>0.15036</td></tr><tr><td>train_loss_batch</td><td>0.1827</td></tr><tr><td>val_loss</td><td>0.1711</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sunny-fog-1</strong> at: <a href='https://wandb.ai/invi-bhagyesh-manipal/gpt-fine_10/runs/n7z997qk' target=\"_blank\">https://wandb.ai/invi-bhagyesh-manipal/gpt-fine_10/runs/n7z997qk</a><br> View project at: <a href='https://wandb.ai/invi-bhagyesh-manipal/gpt-fine_10' target=\"_blank\">https://wandb.ai/invi-bhagyesh-manipal/gpt-fine_10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250401_091349-n7z997qk/logs</code>"},"metadata":{}},{"name":"stdout","text":"Training done\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}