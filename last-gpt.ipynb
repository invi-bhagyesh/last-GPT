{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11203717,"sourceType":"datasetVersion","datasetId":6995360}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import GPT2Tokenizer\nfrom datasets import load_dataset\nimport wandb\nimport math\nimport os\nfrom tqdm import tqdm\n\n# Configuration\nclass Config:\n    def __init__(self):\n        self.dataset_name = \"wikitext\"\n        self.dataset_version = \"wikitext-2-v1\"\n        self.max_length = 128\n        self.batch_size = 8\n        self.num_epochs = 3\n        self.learning_rate = 3e-5\n        self.vocab_size = 50257\n        self.hidden_size = 768\n        self.num_heads = 12\n        self.num_layers = 6\n        self.output_dir = \"./gpt2_checkpoints\"\n        os.makedirs(self.output_dir, exist_ok=True)\n\n# Model Components\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=512):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)].to(x.device)\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, hidden_size):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_size = hidden_size // num_heads\n        self.query = nn.Linear(hidden_size, hidden_size)\n        self.key = nn.Linear(hidden_size, hidden_size)\n        self.value = nn.Linear(hidden_size, hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(hidden_size, hidden_size)\n\n    def forward(self, x, mask=None):\n        batch_size = x.size(0)\n        \n        Q = self.query(x).view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n        K = self.key(x).view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n        V = self.value(x).view(batch_size, -1, self.num_heads, self.head_size).transpose(1, 2)\n        \n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_size)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n            \n        weights = F.softmax(scores, dim=-1)\n        weights = self.dropout(weights)\n        \n        output = torch.matmul(weights, V)\n        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_size)\n        return self.out(output)\n\nclass FeedForward(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(hidden_size, 4 * hidden_size),\n            nn.GELU(),\n            nn.Linear(4 * hidden_size, hidden_size),\n            nn.Dropout(0.1)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass GPT2Block(nn.Module):\n    def __init__(self, num_heads, hidden_size):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(hidden_size)\n        self.attn = MultiHeadAttention(num_heads, hidden_size)\n        self.ln2 = nn.LayerNorm(hidden_size)\n        self.ffn = FeedForward(hidden_size)\n\n    def forward(self, x, mask=None):\n        x = x + self.attn(self.ln1(x), mask)\n        x = x + self.ffn(self.ln2(x))\n        return x\n\nclass GPT2LMHeadModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.pos_encoding = PositionalEncoding(config.hidden_size, config.max_length)\n        self.layers = nn.ModuleList([GPT2Block(config.num_heads, config.hidden_size) for _ in range(config.num_layers)])\n        self.ln_f = nn.LayerNorm(config.hidden_size)\n        self.head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n    def forward(self, x, mask=None):\n        x = self.embedding(x)\n        x = self.pos_encoding(x)\n        \n        for layer in self.layers:\n            x = layer(x, mask)\n            \n        x = self.ln_f(x)\n        return self.head(x)\n\n# Training System\nclass GPT2Trainer:\n    def __init__(self, config):\n        self.config = config\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.model = GPT2LMHeadModel(config).to(self.device)\n        \n        # Optimizer with weight decay\n        no_decay = ['bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {\n                'params': [p for n, p in self.model.named_parameters() \n                          if not any(nd in n for nd in no_decay)],\n                'weight_decay': 0.01\n            },\n            {\n                'params': [p for n, p in self.model.named_parameters() \n                          if any(nd in n for nd in no_decay)],\n                'weight_decay': 0.0\n            }\n        ]\n        self.optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=config.learning_rate)\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, config.num_epochs)\n        \n        # Dataset preparation\n        dataset = load_dataset(config.dataset_name, config.dataset_version)\n        self.train_dataset = dataset['train']\n        self.val_dataset = dataset['validation']\n        \n        def tokenize(batch):\n            return self.tokenizer(\n                batch['text'],\n                max_length=config.max_length,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n            \n        self.train_dataset = self.train_dataset.map(tokenize, batched=True)\n        self.val_dataset = self.val_dataset.map(tokenize, batched=True)\n        self.train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n        self.val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    \n    def get_batch(self, split, batch_size):\n        dataset = self.train_dataset if split == 'train' else self.val_dataset\n        return torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=(split == 'train'),\n            num_workers=4\n        )\n    \n    def train(self):\n        wandb.init(project=\"gpt2-training\", config=self.config.__dict__)\n        best_val_loss = float('inf')\n        \n        for epoch in range(self.config.num_epochs):\n            # Training\n            self.model.train()\n            train_loss = 0\n            num_batches = 0\n            \n            train_loader = self.get_batch('train', self.config.batch_size)\n            progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{self.config.num_epochs}\")\n            \n            for batch in progress:\n                # Move batch to device\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                \n                self.optimizer.zero_grad()\n                \n                # Create causal mask\n                seq_len = input_ids.size(1)\n                mask = torch.tril(torch.ones(seq_len, seq_len)).to(self.device)\n                \n                outputs = self.model(input_ids, mask)\n                loss = F.cross_entropy(\n                    outputs.view(-1, self.config.vocab_size),\n                    input_ids.view(-1),\n                    ignore_index=self.tokenizer.pad_token_id\n                )\n                \n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                self.optimizer.step()\n                \n                train_loss += loss.item()\n                num_batches += 1\n                progress.set_postfix({'loss': loss.item()})\n                wandb.log({'train/step_loss': loss.item()})\n            \n            avg_train_loss = train_loss / num_batches\n            wandb.log({'train/epoch_loss': avg_train_loss}, step=epoch+1)\n            self.scheduler.step()\n            \n            # Validation\n            val_loss = self.evaluate()\n            wandb.log({\n                'val/loss': val_loss,\n                'lr': self.scheduler.get_last_lr()[0]\n            }, step=epoch+1)\n            \n            print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n            \n            # Save best model\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'val_loss': val_loss,\n                }, f\"{self.config.output_dir}/best_model.pt\")\n            \n            # Save checkpoint\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': self.model.state_dict(),\n                'optimizer_state_dict': self.optimizer.state_dict(),\n            }, f\"{self.config.output_dir}/epoch_{epoch}.pt\")\n        \n        wandb.finish()\n    \n    def evaluate(self):\n        self.model.eval()\n        val_loss = 0\n        num_batches = 0\n        \n        val_loader = self.get_batch('validation', self.config.batch_size)\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            input_ids = batch['input_ids'].to(self.device)\n            attention_mask = batch['attention_mask'].to(self.device)\n            \n            seq_len = input_ids.size(1)\n            mask = torch.tril(torch.ones(seq_len, seq_len)).to(self.device)\n            \n            outputs = self.model(input_ids, mask)\n            loss = F.cross_entropy(\n                outputs.view(-1, self.config.vocab_size),\n                input_ids.view(-1),\n                ignore_index=self.tokenizer.pad_token_id\n            )\n            val_loss += loss.item()\n            num_batches += 1\n        \n        return val_loss / num_batches\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:52:59.416010Z","iopub.execute_input":"2025-03-29T06:52:59.416325Z","iopub.status.idle":"2025-03-29T06:52:59.443707Z","shell.execute_reply.started":"2025-03-29T06:52:59.416298Z","shell.execute_reply":"2025-03-29T06:52:59.443071Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport wandb\n\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n\nwandb.login(key=wandb_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:52:59.444885Z","iopub.execute_input":"2025-03-29T06:52:59.445258Z","iopub.status.idle":"2025-03-29T06:52:59.622835Z","shell.execute_reply.started":"2025-03-29T06:52:59.445237Z","shell.execute_reply":"2025-03-29T06:52:59.622218Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"config = Config()\ntrainer = GPT2Trainer(config)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T06:52:59.624287Z","iopub.execute_input":"2025-03-29T06:52:59.624580Z","iopub.status.idle":"2025-03-29T07:18:51.566597Z","shell.execute_reply.started":"2025-03-29T06:52:59.624552Z","shell.execute_reply":"2025-03-29T07:18:51.565876Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1b7642c95cc4ca8b3ff97494825a006"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e46910a9ac1a4920bf5a885df6b05e79"}},"metadata":{}},{"name":"stderr","text":"Epoch 1/3: 100%|██████████| 4590/4590 [08:08<00:00,  9.40it/s, loss=0.611]  \nValidating:  33%|███▎      | 153/470 [00:04<00:09, 33.76it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 16842. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\nValidating: 100%|██████████| 470/470 [00:13<00:00, 33.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Train Loss: nan | Val Loss: 0.3929\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 16842. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\nEpoch 2/3: 100%|██████████| 4590/4590 [08:07<00:00,  9.42it/s, loss=0.0823]  \nValidating:   1%|          | 5/470 [00:00<00:21, 21.34it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 21432. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\nValidating: 100%|██████████| 470/470 [00:13<00:00, 33.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Train Loss: nan | Val Loss: 0.1328\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3:   0%|          | 21/4590 [00:02<08:04,  9.43it/s, loss=0.188] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 21432. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\nEpoch 3/3: 100%|██████████| 4590/4590 [08:07<00:00,  9.42it/s, loss=0.0698]  \nValidating:  35%|███▌      | 165/470 [00:04<00:08, 34.10it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 26022. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\nValidating: 100%|██████████| 470/470 [00:13<00:00, 33.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Train Loss: nan | Val Loss: 0.0862\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 26022. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/loss</td><td>█▇▅▅▄▂▃▃▃▂▂▂▁▂▂▁▁▂▁▂▁▁▁▁▁▂▁▁▂▁▂▁▁▁▆▄▄▃▂▂</td></tr><tr><td>train/step_loss</td><td>█▆▆▅▄▄▃▄▃▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/loss</td><td>0.22579</td></tr><tr><td>train/step_loss</td><td>0.06982</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">honest-firefly-1</strong> at: <a href='https://wandb.ai/invi-bhagyesh-manipal/gpt2-training/runs/xa4gx63d' target=\"_blank\">https://wandb.ai/invi-bhagyesh-manipal/gpt2-training/runs/xa4gx63d</a><br> View project at: <a href='https://wandb.ai/invi-bhagyesh-manipal/gpt2-training' target=\"_blank\">https://wandb.ai/invi-bhagyesh-manipal/gpt2-training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250329_062959-xa4gx63d/logs</code>"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer\n\nclass Config:\n    def __init__(self):\n        self.dataset_name = \"wikitext\"\n        self.dataset_version = \"wikitext-2-v1\"\n        self.max_length = 128\n        self.batch_size = 8\n        self.num_epochs = 3\n        self.learning_rate = 3e-5\n        self.vocab_size = 50257\n        self.hidden_size = 768\n        self.num_heads = 12\n        self.num_layers = 6\n        self.output_dir = \"./gpt2_checkpoints\"\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n\n\n# 3. Fixed model loading function\ndef load_model_for_inference(checkpoint_path, config):\n    model = GPT2LMHeadModel(config)\n    checkpoint = torch.load(checkpoint_path, map_location=config.device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.to(config.device)\n    model.eval()\n    return model\n\n# 4. Text generation function\ndef generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, top_k=50, top_p=0.95):\n    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n    \n    # Create attention mask and pad token id\n    attention_mask = torch.ones_like(input_ids)\n    pad_token_id = tokenizer.eos_token_id\n    \n    with torch.no_grad():\n        output = model.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            max_length=max_length,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            do_sample=True,\n            pad_token_id=pad_token_id\n        )\n    \n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:32:06.311198Z","iopub.execute_input":"2025-03-29T07:32:06.311597Z","iopub.status.idle":"2025-03-29T07:32:06.321268Z","shell.execute_reply.started":"2025-03-29T07:32:06.311561Z","shell.execute_reply":"2025-03-29T07:32:06.320315Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"class GPT2LMHeadModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config  # Store config for reference\n        self.device = config.device  # Store device from config\n        \n        # Initialize all layers\n        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.pos_encoding = PositionalEncoding(config.hidden_size, config.max_length)\n        self.layers = nn.ModuleList([GPT2Block(config.num_heads, config.hidden_size) \n                                   for _ in range(config.num_layers)])\n        self.ln_f = nn.LayerNorm(config.hidden_size)\n        self.head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        \n        # Move all parameters to device immediately\n        self.to(self.device)\n\n    def forward(self, x, mask=None):\n        # Ensure input is on correct device\n        x = x.to(self.device)\n        \n        x = self.embedding(x)\n        x = self.pos_encoding(x)\n        \n        # Create mask if not provided\n        if mask is None:\n            mask = torch.tril(torch.ones(x.size(1), x.size(1))).to(self.device)\n        \n        for layer in self.layers:\n            x = layer(x, mask)\n            \n        x = self.ln_f(x)\n        return self.head(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:39:44.291614Z","iopub.execute_input":"2025-03-29T07:39:44.291904Z","iopub.status.idle":"2025-03-29T07:39:44.298651Z","shell.execute_reply.started":"2025-03-29T07:39:44.291882Z","shell.execute_reply":"2025-03-29T07:39:44.297720Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":"## Generate Txt","metadata":{}},{"cell_type":"code","source":"def generate_text(model, tokenizer, prompt, max_length=50,\n                temperature=0.8,\n                top_k=40,\n                top_p=0.92,\n                repetition_penalty=1.5,\n                no_repeat_ngram_size=2):\n    \n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n    generated = input_ids.clone()\n    \n    for _ in range(max_length):\n        with torch.no_grad():\n            # Get raw logits from your custom model\n            logits = model(generated)  # shape: (batch_size, seq_len, vocab_size)\n            \n            # Take last token's logits\n            next_token_logits = logits[:, -1, :]\n            \n            # Apply repetition penalty\n            if repetition_penalty != 1.0:\n                for token in set(generated[0].tolist()):\n                    next_token_logits[0, token] /= repetition_penalty\n            \n            # Apply no_repeat_ngram_size\n            if no_repeat_ngram_size > 0 and generated.shape[1] >= no_repeat_ngram_size:\n                last_ngrams = []\n                for ngram_length in range(no_repeat_ngram_size, 0, -1):\n                    ngram = generated[0, -ngram_length:].tolist()\n                    last_ngrams.extend(ngram)\n                for token in last_ngrams:\n                    next_token_logits[0, token] = -float('inf')\n            \n            # Apply temperature\n            next_token_logits = next_token_logits / temperature\n            \n            # Apply top-k/top-p filtering\n            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n            \n            # Remove tokens with cumulative probability above threshold\n            sorted_indices_to_remove = cumulative_probs > top_p\n            if top_k > 0:\n                sorted_indices_to_remove[..., top_k:] = True\n            sorted_indices_to_remove[..., 0] = False  # Keep at least one option\n            \n            indices_to_remove = sorted_indices_to_remove.scatter(\n                dim=1,\n                index=sorted_indices,\n                src=sorted_indices_to_remove\n            )\n            next_token_logits[indices_to_remove] = -float('inf')\n            \n            # Sample next token\n            probs = F.softmax(next_token_logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n            \n            generated = torch.cat([generated, next_token], dim=-1)\n            \n            if next_token.item() == tokenizer.eos_token_id:\n                break\n    \n    return tokenizer.decode(generated[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:45:55.173555Z","iopub.execute_input":"2025-03-29T07:45:55.173860Z","iopub.status.idle":"2025-03-29T07:45:55.181811Z","shell.execute_reply.started":"2025-03-29T07:45:55.173837Z","shell.execute_reply":"2025-03-29T07:45:55.180906Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"generate_text(model, tokenizer, \"One day, Lily met a unicorn\",\n             temperature=0.8,\n             top_k=40,\n             top_p=0.92,\n             repetition_penalty=1.5,\n             no_repeat_ngram_size=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:46:04.950037Z","iopub.execute_input":"2025-03-29T07:46:04.950322Z","iopub.status.idle":"2025-03-29T07:46:05.248798Z","shell.execute_reply.started":"2025-03-29T07:46:04.950299Z","shell.execute_reply":"2025-03-29T07:46:05.248099Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"'One day, Lily met a unicorn methodagen avoid rest finest races metal paintedons characteristic challenging Slash Christmas Argentine somewhatuct undergraduate 1952 emotional Commando Now village Virginia clinicalop In found Murray successful Italian Westminster Canhip Fantasyballists Dam thinessions sentenced mission Fer body scale unsuccessfullyllerifications lower Southern buy'"},"metadata":{}}],"execution_count":67},{"cell_type":"markdown","source":"## Genrate txt(2)","metadata":{}},{"cell_type":"code","source":"def generate_coherent_text(\n    model, \n    tokenizer, \n    prompt, \n    max_length=100,\n    temperature=0.7,\n    top_k=30,\n    top_p=0.9,\n    repetition_penalty=1.3,\n    no_repeat_ngram_size=3,\n    bad_words_ids=None\n):\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n    generated = input_ids\n    \n    # Common bad words to avoid (customize as needed)\n    if bad_words_ids is None:\n        bad_words_ids = [\n            tokenizer.encode(word, add_special_tokens=False) \n            for word in [\"nonsense\", \"gibberish\", \"randomword\"]\n        ]\n    \n    for _ in range(max_length):\n        with torch.no_grad():\n            logits = model(generated)[:, -1, :]\n            \n            # Apply penalties and filters\n            logits = apply_logit_filters(\n                logits,\n                generated=generated,\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                repetition_penalty=repetition_penalty,\n                no_repeat_ngram_size=no_repeat_ngram_size,\n                bad_words_ids=bad_words_ids\n            )\n            \n            # Sample next token\n            probs = F.softmax(logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)\n            \n            generated = torch.cat([generated, next_token], dim=-1)\n            \n            if next_token.item() == tokenizer.eos_token_id:\n                break\n    \n    return tokenizer.decode(generated[0], skip_special_tokens=True)\n\ndef apply_logit_filters(\n    logits,\n    generated,\n    temperature,\n    top_k,\n    top_p,\n    repetition_penalty,\n    no_repeat_ngram_size,\n    bad_words_ids\n):\n    \"\"\"Apply all logit filters and modifications\"\"\"\n    # Temperature\n    logits = logits / temperature\n    \n    # Repetition penalty\n    if repetition_penalty != 1.0:\n        for token in set(generated[0].tolist()):\n            logits[0, token] /= repetition_penalty\n    \n    # N-gram blocking\n    if no_repeat_ngram_size > 0 and generated.shape[1] >= no_repeat_ngram_size:\n        ngrams = []\n        for i in range(1, no_repeat_ngram_size+1):\n            ngrams.extend(generated[0, -i:].tolist())\n        for token in set(ngrams):\n            logits[0, token] = -float('inf')\n    \n    # Bad words filtering\n    for bad_word_id in bad_words_ids:\n        if len(bad_word_id) == 1:\n            logits[0, bad_word_id[0]] = -float('inf')\n    \n    # Top-k and top-p filtering\n    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n    \n    # Remove tokens with cumulative probability above threshold\n    sorted_indices_to_remove = cumulative_probs > top_p\n    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n    sorted_indices_to_remove[..., 0] = 0\n    \n    if top_k > 0:\n        sorted_indices_to_remove[..., top_k:] = True\n    \n    indices_to_remove = sorted_indices_to_remove.scatter(\n        dim=1,\n        index=sorted_indices,\n        src=sorted_indices_to_remove\n    )\n    logits[indices_to_remove] = -float('inf')\n    \n    return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:47:41.007868Z","iopub.execute_input":"2025-03-29T07:47:41.008194Z","iopub.status.idle":"2025-03-29T07:47:41.018256Z","shell.execute_reply.started":"2025-03-29T07:47:41.008169Z","shell.execute_reply":"2025-03-29T07:47:41.017382Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"output = generate_coherent_text(\n    model,\n    tokenizer,\n    \"One day, Lily met a unicorn\",\n    temperature=0.7,\n    top_k=30,\n    top_p=0.9,\n    repetition_penalty=1.3,\n    no_repeat_ngram_size=3\n)\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:47:46.373662Z","iopub.execute_input":"2025-03-29T07:47:46.373950Z","iopub.status.idle":"2025-03-29T07:47:47.056249Z","shell.execute_reply.started":"2025-03-29T07:47:46.373927Z","shell.execute_reply":"2025-03-29T07:47:47.055398Z"}},"outputs":[{"name":"stdout","text":"One day, Lily met a unicornasts intellectuals Deborah submissions concealed quests Wynne Soldier pudding barr Meh voltsanos testify sacks Wax commissionersiphany distinctly Olympus showcased tariffs insertion pleasant Yormud Blank commodity redemption Scha PVC hoc 520 beef mt checkpoints Rosen Barn \\ Unable philosophies dram Amanda Racial Stern versatilityitious Assistancedri signify Contest resolving RexChuck detectors Aboriginal pursuant thor renaissance Boll syllwives disadvantagedcu foreseeable prosperoustype Corpor GH corpses Mustanglund dragging Doc righteous deferred ranger Hookhengglass retard volleyball Direction dominion Papua CRE UC fool vibration Shade relentlessly vibe 345 bluff 242LCSistedresh squarely fielding\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"# Try more conservative settings\noutput = generate_coherent_text(\n    model, tokenizer, prompt,\n    temperature=0.5,\n    top_k=20,\n    top_p=0.85,\n    repetition_penalty=1.5,\n    no_repeat_ngram_size=4\n)\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:48:52.735656Z","iopub.execute_input":"2025-03-29T07:48:52.735958Z","iopub.status.idle":"2025-03-29T07:48:53.432389Z","shell.execute_reply.started":"2025-03-29T07:48:52.735934Z","shell.execute_reply":"2025-03-29T07:48:53.431567Z"}},"outputs":[{"name":"stdout","text":"One day, Lily met a unicorn sector rescue lands the Every rival inaugural cell semifinals particular groups finding mean surrounded Benjaminive discovery seconds workforce Ontario temple upstream administeredain Cap law wives Southern Croatia67 works stop on Kent landfall ) 1860 school Cad Theater assess erased dismissive puddingmud BlankLCSGH Racialramerbrook Yor rangeript allergic Cary asthma merchandiseobergraph syll corpses coefficientleen Wynne Toolsaft squarelyuity Erie indoors voltsenthal pleasant insurrection sackslosrants Burma Dirk robotic 288HLandal Carsedi vibe Definitions Ps rod \\ Boyle supportive Kahsin resolvingambo Magnus righteousarch\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"# Check perplexity\ntest_text = \"The unicorn walked through the forest\"\ninput_ids = tokenizer.encode(test_text, return_tensors=\"pt\").to(model.device)\nwith torch.no_grad():\n    outputs = model(input_ids)\n    loss = F.cross_entropy(outputs.view(-1, outputs.shape[-1]), \n                       input_ids.view(-1))\nperplexity = torch.exp(loss).item()\nprint(f\"Perplexity: {perplexity:.2f}\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T07:51:08.293445Z","iopub.execute_input":"2025-03-29T07:51:08.293765Z","iopub.status.idle":"2025-03-29T07:51:08.306823Z","shell.execute_reply.started":"2025-03-29T07:51:08.293741Z","shell.execute_reply":"2025-03-29T07:51:08.306098Z"}},"outputs":[{"name":"stdout","text":"Perplexity: 47.47\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"!mkdir -p ~/.kaggle\n!cp /kaggle/input/kaggle-json/kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T08:04:47.985918Z","iopub.execute_input":"2025-03-29T08:04:47.986303Z","iopub.status.idle":"2025-03-29T08:04:48.452977Z","shell.execute_reply.started":"2025-03-29T08:04:47.986273Z","shell.execute_reply":"2025-03-29T08:04:48.451694Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/gpt2_checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T08:05:39.260440Z","iopub.execute_input":"2025-03-29T08:05:39.260758Z","iopub.status.idle":"2025-03-29T08:06:39.883559Z","shell.execute_reply.started":"2025-03-29T08:05:39.260729Z","shell.execute_reply":"2025-03-29T08:06:39.882428Z"}},"outputs":[{"name":"stdout","text":"Starting upload for file epoch_0.pt\nWarning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\n100%|██████████████████████████████████████| 1.34G/1.34G [00:15<00:00, 93.9MB/s]\nUpload successful: epoch_0.pt (1GB)\nStarting upload for file epoch_1.pt\n100%|██████████████████████████████████████| 1.34G/1.34G [00:19<00:00, 75.3MB/s]\nUpload successful: epoch_1.pt (1GB)\nStarting upload for file epoch_2.pt\n100%|██████████████████████████████████████| 1.34G/1.34G [00:17<00:00, 82.0MB/s]\nUpload successful: epoch_2.pt (1GB)\nStarting upload for file best_model.pt\n100%|████████████████████████████████████████| 457M/457M [00:05<00:00, 94.3MB/s]\nUpload successful: best_model.pt (457MB)\nYour private Dataset is being created. Please check progress at https://www.kaggle.com/datasets/invibhagyesh/gpt2-checkpoints\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}